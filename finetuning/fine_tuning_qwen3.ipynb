{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83f9bad5-f58b-4dcf-bcae-e7c8a68180c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install -U accelerate \n",
    "%pip install -U peft \n",
    "%pip install -U trl \n",
    "%pip install --upgrade bitsandbytes\n",
    "%pip install -U transformers\n",
    "%pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afe2db1-f238-4f40-8c7f-013c5b836804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "hf_token = \"\"\n",
    "\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de1c28d6-b235-4344-b236-c8dc62cf7e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20395517f0ba46b996cb9acf843fcac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/9.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e078cd710145e7921d188127134b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ed617212754dfe92361cf28a8fa7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fb110a11c5428396e908aefdbb2940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef23397f32444458e8897e0a55038a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/729 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dab0ad77bdf4e2c91feae52510423eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/32.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "469283753c3e4b0fa67e4c4ebb3195cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4277ae30af1e4787bb2ee81a4968c045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00005.safetensors:   0%|          | 0.00/4.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98724e3377441c7abe5fb680a38aa32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00005.safetensors:   0%|          | 0.00/3.19G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d52a465f89a4de7847988d56b0f431a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00005.safetensors:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2679feef3274848b48ae6245ab72ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00005.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c383403673f413e995890d2908690c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00005.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8635de5aa2d8457c8fee2198d3d6c5a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0f5a3f8087461d951fe1326e2c9f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "import json, random\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_dir = \"Qwen/Qwen3-8B-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    quantization_config=bnb_config,   \n",
    "    device_map=\"auto\",  \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True             \n",
    ")\n",
    "\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a924a4-0a53-42c1-9fd7-815eb36bbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is a fixed instruction that guides the assistant to write a Korean patent specification in full compliance with Korean Patent Law. \n",
    "The assistant must use the provided invention details to generate a structured patent section in Korean.\n",
    "Think step-by-step before responding\n",
    "The final response should be written in Korean and follow the 2007 model specification guidelines.\n",
    "**Respond in Korean.**\n",
    "\n",
    "### Task Type: {task_type}\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Output:\n",
    "{output}\"\"\"\n",
    "\n",
    "# Í∞Å ÌÉúÏä§ÌÅ¨Î≥Ñ instruction Ï†ïÏùò\n",
    "TASK_INSTRUCTIONS = {\n",
    "    \"PATENT_FORM\": \"\"\"ÎãπÏã†ÏùÄ ÎåÄÌïúÎØºÍµ≠ ÌäπÌóàÎ≤ïÏóê Îî∞Îùº Î™ÖÏÑ∏ÏÑúÎ•º ÏûëÏÑ±ÌïòÎäî ÌäπÌóà Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. ÏïÑÎûòÏùò Íµ¨ÏÑ±ÏöîÏÜåÎ≥ÑÎ°ú Ï†ïÌôïÌïòÍ≥† Íµ¨Ï°∞ÌôîÎêú Î¨∏ÏÑúÎ•º ÏÉùÏÑ±Ìï¥Ïïº Ìï©ÎãàÎã§. \n",
    "Î™®Îì† Ìï≠Î™©ÏùÄ ÌäπÌóàÎ≤ï Ï†ú42Ï°∞ Î∞è ÏãúÌñâÍ∑úÏπô Ï†ú21Ï°∞, Í∞úÏ†ïÎêú Î™®Î≤î Î™ÖÏÑ∏ÏÑú ÏûëÏÑ±Î≤ï(2007.07.01. Ïù¥ÌõÑ Ï†ÅÏö©)ÏùÑ Ï≤†Ï†ÄÌûà Ï§ÄÏàòÌï¥Ïïº Ìï©ÎãàÎã§.\n",
    "\n",
    "1. [Î∞úÎ™ÖÏùò Î™ÖÏπ≠] - Î∞úÎ™ÖÏùò ÎÇ¥Ïö©ÏùÑ Í∞ÑÎ™ÖÌïòÍ≤å ÌëúÌòÑÌïòÎäî Î™ÖÏπ≠ÏùÑ ÏûëÏÑ±Ìï©ÎãàÎã§. ÏòÅÎ¨∏Î™ÖÏùÑ {{}} ÏïàÏóê Ìï®Íªò Í∏∞Ïû¨Ìï©ÎãàÎã§.\n",
    "2. [Í∏∞Ïà†Î∂ÑÏïº] - Î≥∏ Î∞úÎ™ÖÏù¥ ÏÜçÌïòÎäî Í∏∞Ïà†Î∂ÑÏïºÎ•º Í∞ÑÍ≤∞ÌïòÍ≤å ÏÑ§Î™ÖÌï©ÎãàÎã§.\n",
    "3. [Î∞∞Í≤ΩÍ∏∞Ïà†] - Ï¢ÖÎûò Í∏∞Ïà†ÏùÑ ÏÑúÏà†ÌïòÍ≥†, Í∞ÄÎä•ÌïòÎ©¥ Î¨∏Ìóå Ïù∏Ïö©ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.\n",
    "4. [Ìï¥Í≤∞ÌïòÎ†§Îäî Í≥ºÏ†ú] - Í∏∞Ï°¥ Í∏∞Ïà†Ïùò Î¨∏Ï†úÏ†êÍ≥º Ìï¥Í≤∞ÌïòÍ≥†Ïûê ÌïòÎäî Í≥ºÏ†úÎ•º Í∏∞Ïà†Ìï©ÎãàÎã§.\n",
    "5. [Í≥ºÏ†úÏùò Ìï¥Í≤∞ ÏàòÎã®] - Î≥∏ Î∞úÎ™ÖÏù¥ Í≥ºÏ†úÎ•º Ïñ¥ÎñªÍ≤å Ìï¥Í≤∞ÌïòÎäîÏßÄÎ•º Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Í∏∞Ïà†ÌïòÍ≥†, Ï≤≠Íµ¨Ìï≠Í≥º Ïó∞Í≤∞Ìï©ÎãàÎã§.\n",
    "6. [Î∞úÎ™ÖÏùò Ìö®Í≥º] - Î≥∏ Î∞úÎ™ÖÏù¥ Ï†úÍ≥µÌïòÎäî Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† ÎπÑÍµê Í∞ÄÎä•Ìïú Í∏∞Ïà†Ï†Å Ìö®Í≥ºÎ•º Í∏∞Ïà†Ìï©ÎãàÎã§.\n",
    "7. [Î∞úÎ™ÖÏùÑ Ïã§ÏãúÌïòÍ∏∞ ÏúÑÌïú Íµ¨Ï≤¥Ï†ÅÏù∏ ÎÇ¥Ïö©] - Ïã§ÏãúÏòà, ÏàòÎã®, ÏàòÏπò Îì±ÏùÑ Ìè¨Ìï®ÌïòÏó¨, ÌèâÍ∑† Í∏∞Ïà†ÏûêÍ∞Ä Ïû¨ÌòÑ Í∞ÄÎä•ÌïòÎèÑÎ°ù ÏûëÏÑ±Ìï©ÎãàÎã§.\n",
    "8. [ÎèÑÎ©¥Ïùò Í∞ÑÎã®Ìïú ÏÑ§Î™Ö] - ÎèÑÎ©¥Ïù¥ ÏûàÏùÑ Í≤ΩÏö∞, Í∞Å ÎèÑÎ©¥Ïù¥ Î¨¥ÏóáÏùÑ ÎÇòÌÉÄÎÇ¥ÎäîÏßÄ Í∏∞Ïà†Ìï©ÎãàÎã§.\n",
    "9. [ÌäπÌóàÏ≤≠Íµ¨Î≤îÏúÑ] - ÎèÖÎ¶ΩÌï≠ Î∞è Ï¢ÖÏÜçÌï≠ÏùÑ Î≤àÌò∏ÏôÄ Ìï®Íªò Î™ÖÌôïÌûà Íµ¨Î∂ÑÌïòÏó¨ ÏûëÏÑ±Ìï©ÎãàÎã§.\"\"\",\n",
    "\n",
    "    \"PATENT_EVALUATION\": \"\"\"Ï†úÏ∂úÎêú ÌäπÌóà Î¨∏ÏÑú ÎòêÎäî ÏïÑÏù¥ÎîîÏñ¥Î•º ÌèâÍ∞ÄÌïòÍ≥† Í∞úÏÑ† Î∞©Ìñ•ÏùÑ Ï†úÏïàÌïòÏÑ∏Ïöî. \n",
    "Îã§Ïùå Í∏∞Ï§ÄÏóê Îî∞Îùº Î∂ÑÏÑùÌïòÍ≥† Ï†êÏàòÏôÄ Íµ¨Ï≤¥Ï†ÅÏù∏ Í∞úÏÑ†ÏÇ¨Ìï≠ÏùÑ Ï†úÏãúÌïòÏÑ∏Ïöî:\n",
    "1. [Ïã†Í∑úÏÑ±] - ÎèôÏùºÌïú Î∞úÎ™ÖÏù¥ Ïù¥ÎØ∏ Íµ≠ÎÇ¥Ïô∏Ïóê Í≥µÍ∞úÎêú Í≤ΩÏö∞ ÌäπÌóàÎ•º Î∞õÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\n",
    "2. [ÏßÑÎ≥¥ÏÑ±] - Ìï¥Îãπ Í∏∞Ïà†Ïù¥ ÌÜµÏÉÅÏùò Í∏∞Ïà†ÏûêÏóêÍ≤å ÏûêÎ™ÖÌïú Í≤ΩÏö∞ ÌäπÌóàÎ•º Î∞õÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.\n",
    "3. [ÏÇ∞ÏóÖÏ†Å Ïù¥Ïö© Í∞ÄÎä•ÏÑ±] - Î∞úÎ™ÖÏù¥ ÏÇ∞ÏóÖÏ†ÅÏúºÎ°ú ÌôúÏö© Í∞ÄÎä•Ìï¥Ïïº Ìï©ÎãàÎã§.\n",
    "4. [Í∏∞Ïû¨Î∂àÎπÑ] - Î™ÖÏÑ∏ÏÑúÏóê Î∞úÎ™ÖÏùò ÎÇ¥Ïö©ÏùÑ Î™ÖÌôïÌïòÍ≥† ÏôÑÏ†ÑÌïòÍ≤å Í∏∞Ïû¨Ìï¥Ïïº ÌïòÎ©∞, ÌÜµÏÉÅÏùò Í∏∞Ïà†ÏûêÍ∞Ä Ïù¥Î•º Ïû¨ÌòÑÌï† Ïàò ÏûàÏñ¥Ïïº Ìï©ÎãàÎã§.\n",
    "Í∞Å Ìï≠Î™©Î≥ÑÎ°ú Ï†êÏàò(1-10Ï†ê)ÏôÄ ÏÉÅÏÑ∏Ìïú ÌèâÍ∞Ä ÏùòÍ≤¨ÏùÑ Ï†úÍ≥µÌïòÏÑ∏Ïöî.\"\"\",\n",
    "\n",
    "#     \"PATENT_RECOMMENDATION\": \"\"\"ÌèâÍ∞ÄÏóêÏÑú ÏßÄÏ†ÅÎêú Î∂ÄÎ∂ÑÏùÑ Î∞òÏòÅÌïòÏó¨ Î™ÖÏÑ∏ÏÑúÎ•º Í∞úÏÑ†ÌïòÎêò, Î™®Îì† Ìï≠Î™©(Ï†úÎ™© Ìè¨Ìï®)ÏùÑ ÏÉùÎûµÌïòÏßÄ ÎßêÍ≥† Í∑∏ÎåÄÎ°ú Ïú†ÏßÄÌïòÏÑ∏Ïöî. \n",
    "# Ìï≠Î™© Ïù¥Î¶Ñ, ÏàúÏÑú, Íµ¨ÏÑ±ÏùÄ Ïú†ÏßÄÌïòÎêò, Í∞Å Ìï≠Î™©Ïùò Î¨∏Ïû•Ïù¥ÎÇò ÎÇ¥Ïö©ÏùÑ ÌèâÍ∞ÄÏóê Îî∞Îùº Î≥¥ÏôÑÌïòÍ±∞ÎÇò Í∞úÏÑ†ÌïòÏÑ∏Ïöî.\n",
    "# Í∏∞Ï°¥ Î™ÖÏÑ∏ÏÑúÏùò Ï†ÑÏ≤¥ Íµ¨Ï°∞Î•º Ïú†ÏßÄÌïòÎ©¥ÏÑú ÌèâÍ∞Ä ÏùòÍ≤¨ÏùÑ Î∞òÏòÅÌïú ÏàòÏ†ïÎêú Ï†ÑÏ≤¥ Î™ÖÏÑ∏ÏÑúÎ•º Ï†úÍ≥µÌïòÏÑ∏Ïöî.\"\"\",\n",
    "\n",
    "    \"PATENT_MODIFICATION\": \"\"\"ÎãπÏã†ÏùÄ ÌäπÌóà Î™ÖÏÑ∏ÏÑú ÏûëÏÑ± Ï†ÑÎ¨∏Í∞ÄÏûÖÎãàÎã§. \n",
    "ÏÇ¨Ïö©ÏûêÏùò ÏöîÏ≤≠Ïóê Îî∞Îùº ÌäπÌóà Î™ÖÏÑ∏ÏÑúÎ•º Ï†ÑÎ¨∏Ï†ÅÏù¥Í≥† Ï†ïÌôïÌïòÍ≤å ÏàòÏ†ïÌïòÎ©∞, ÏÑπÏÖò Í∞ÑÏùò ÏùºÍ¥ÄÏÑ±ÏùÑ Ïú†ÏßÄÌï©ÎãàÎã§.\n",
    "ÏàòÏ†ï Ïãú ÌäπÌóàÎ≤ï Î∞è Í¥ÄÎ†® Í∑úÏ†ïÏùÑ Ï§ÄÏàòÌïòÍ≥†, Í∏∞Ïà†Ï†Å Ï†ïÌôïÏÑ±Í≥º Î≤ïÏ†Å Ïú†Ìö®ÏÑ±ÏùÑ ÌôïÎ≥¥ÌïòÏÑ∏Ïöî.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8648f94f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading multi-task data...\n",
      "Loading data/patent_data.jsonl for PATENT_FORM...\n",
      "  Loaded 1000 samples for PATENT_FORM\n",
      "Loading data/evaluation_data.jsonl for PATENT_EVALUATION...\n",
      "  Loaded 1997 samples for PATENT_EVALUATION\n",
      "Loading data/ai_revise_data.jsonl for PATENT_MODIFICATION...\n",
      "  Loaded 3022 samples for PATENT_MODIFICATION\n",
      "\n",
      "Total unified data: 6019\n",
      "Task distribution:\n",
      "  PATENT_FORM: 1000\n",
      "  PATENT_EVALUATION: 1997\n",
      "  PATENT_MODIFICATION: 3022\n",
      "\n",
      "üîÑ Creating balanced dataset...\n",
      "PATENT_EVALUATION: 2000 samples (original: 1997)\n",
      "PATENT_MODIFICATION: 2000 samples (original: 3022)\n",
      "PATENT_FORM: 2000 samples (original: 1000)\n",
      "\n",
      "‚úÖ Final balanced dataset size: 6000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a23bc18a2704908bceb925f017e6207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset after prompt formatting: 6000\n",
      "\n",
      "üìã Sample data preview:\n",
      "--------------------------------------------------\n",
      "Below is a fixed instruction that guides the assistant to work as a Korean patent AI assistant.\n",
      "The assistant must identify the task type and respond accordingly in Korean.\n",
      "Think step-by-step before responding.\n",
      "The final response should be written in Korean.\n",
      "**Respond in Korean.**\n",
      "\n",
      "### Task Type: PATENT_FORM\n",
      "\n",
      "### Instruction:\n",
      "ÎãπÏã†ÏùÄ ÎåÄÌïúÎØºÍµ≠ ÌäπÌóàÎ≤ïÏóê Îî∞Îùº Î™ÖÏÑ∏ÏÑúÎ•º ÏûëÏÑ±ÌïòÎäî ÌäπÌóà Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏ÏûÖÎãàÎã§. ÏïÑÎûòÏùò Íµ¨ÏÑ±ÏöîÏÜåÎ≥ÑÎ°ú Ï†ïÌôïÌïòÍ≥† Íµ¨Ï°∞ÌôîÎêú Î¨∏ÏÑúÎ•º ÏÉùÏÑ±Ìï¥Ïïº Ìï©ÎãàÎã§. \n",
      "Î™®Îì† Ìï≠Î™©ÏùÄ ÌäπÌóàÎ≤ï Ï†ú42Ï°∞ Î∞è ÏãúÌñâÍ∑úÏπô Ï†ú21Ï°∞, Í∞úÏ†ïÎêú Î™®Î≤î Î™ÖÏÑ∏ÏÑú ÏûëÏÑ±Î≤ï(2007.07.01. Ïù¥ÌõÑ Ï†ÅÏö©)ÏùÑ Ï≤†Ï†ÄÌûà Ï§ÄÏàòÌï¥Ïïº Ìï©ÎãàÎã§.\n",
      "\n",
      "1. [Î∞úÎ™ÖÏùò Î™ÖÏπ≠] - Î∞úÎ™ÖÏùò ÎÇ¥Ïö©ÏùÑ Í∞ÑÎ™ÖÌïòÍ≤å ÌëúÌòÑÌïòÎäî Î™ÖÏπ≠ÏùÑ ÏûëÏÑ±Ìï©ÎãàÎã§. ÏòÅÎ¨∏Î™ÖÏùÑ {{}} ÏïàÏóê Ìï®Íªò Í∏∞Ïû¨Ìï©ÎãàÎã§.\n",
      "2. [Í∏∞Ïà†Î∂ÑÏïº] - Î≥∏ Î∞úÎ™ÖÏù¥ ÏÜçÌïòÎäî Í∏∞Ïà†Î∂ÑÏïºÎ•º Í∞ÑÍ≤∞ÌïòÍ≤å ÏÑ§Î™ÖÌï©ÎãàÎã§.\n",
      "3. [Î∞∞Í≤ΩÍ∏∞Ïà†] - Ï¢ÖÎûò Í∏∞Ïà†ÏùÑ ÏÑúÏà†ÌïòÍ≥†, Í∞ÄÎä•ÌïòÎ©¥ Î¨∏Ìóå Ïù∏Ïö©ÏùÑ Ìè¨Ìï®Ìï©ÎãàÎã§.\n",
      "4. [Ìï¥Í≤∞ÌïòÎ†§Îäî Í≥ºÏ†ú] - Í∏∞Ï°¥ Í∏∞Ïà†Ïùò Î¨∏Ï†úÏ†êÍ≥º Ìï¥Í≤∞ÌïòÍ≥†Ïûê ÌïòÎäî Í≥ºÏ†úÎ•º Í∏∞Ïà†Ìï©ÎãàÎã§.\n",
      "5. [Í≥ºÏ†úÏùò Ìï¥Í≤∞ ÏàòÎã®] - Î≥∏ Î∞úÎ™ÖÏù¥ Í≥ºÏ†úÎ•º Ïñ¥ÎñªÍ≤å Ìï¥Í≤∞ÌïòÎäîÏßÄÎ•º Íµ¨Ï≤¥Ï†ÅÏúºÎ°ú Í∏∞Ïà†ÌïòÍ≥†, Ï≤≠Íµ¨Ìï≠Í≥º Ïó∞Í≤∞Ìï©ÎãàÎã§.\n",
      "6. [Î∞úÎ™ÖÏùò Ìö®Í≥º] - Î≥∏ Î∞úÎ™ÖÏù¥ Ï†úÍ≥µÌïòÎäî Íµ¨Ï≤¥Ï†ÅÏù¥Í≥† ÎπÑÍµê Í∞ÄÎä•Ìïú Í∏∞Ïà†Ï†Å Ìö®Í≥ºÎ•º Í∏∞Ïà†Ìï©ÎãàÎã§.\n",
      "7. [...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "import json\n",
    "import random\n",
    "from datasets import Dataset\n",
    "\n",
    "# ÌÜµÌï© ÌîÑÎ°¨ÌîÑÌä∏ ÌÖúÌîåÎ¶ø (Cell 4ÏóêÏÑú Î≥µÏÇ¨)\n",
    "unified_prompt_style = \"\"\"Below is a fixed instruction that guides the assistant to work as a Korean patent AI assistant.\n",
    "The assistant must identify the task type and respond accordingly in Korean.\n",
    "Think step-by-step before responding.\n",
    "The final response should be written in Korean.\n",
    "**Respond in Korean.**\n",
    "\n",
    "### Task Type: {task_type}\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Context:\n",
    "{context}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Output:\n",
    "{output}\"\"\"\n",
    "\n",
    "def load_jsonl_dataset(path):\n",
    "    \"\"\"JSONL ÌååÏùºÏùÑ Î°úÎìúÌïòÏó¨ Î¶¨Ïä§Ìä∏Î°ú Î∞òÌôò\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "    return data\n",
    "\n",
    "def load_and_process_multi_task_data():\n",
    "    \"\"\"3Í∞úÏùò ÌÉúÏä§ÌÅ¨ Îç∞Ïù¥ÌÑ∞Î•º Î°úÎìúÌïòÍ≥† ÌÜµÌï© (Ï∂îÏ≤ú Îç∞Ïù¥ÌÑ∞ Ï†úÏô∏)\"\"\"\n",
    "    \n",
    "    # ÌååÏùºÎ™ÖÍ≥º ÌÉúÏä§ÌÅ¨ ÌÉÄÏûÖ Îß§Ìïë (Ï∂îÏ≤ú Îç∞Ïù¥ÌÑ∞ Ï£ºÏÑùÏ≤òÎ¶¨)\n",
    "    file_task_mapping = {\n",
    "        \"data/patent_data.jsonl\": \"PATENT_FORM\",\n",
    "        \"data/evaluation_data.jsonl\": \"PATENT_EVALUATION\", \n",
    "        # \"recomend_data.jsonl\": \"PATENT_RECOMMENDATION\",  # TODO: Ï∂îÏ≤ú Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑÎêòÎ©¥ Ï£ºÏÑù Ìï¥Ï†ú\n",
    "        \"data/ai_revise_data.jsonl\": \"PATENT_MODIFICATION\"\n",
    "    }\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for filename, task_type in file_task_mapping.items():\n",
    "        print(f\"Loading {filename} for {task_type}...\")\n",
    "        \n",
    "        try:\n",
    "            # JSONL ÌååÏùº Î°úÎìú\n",
    "            task_data = load_jsonl_dataset(filename)\n",
    "            \n",
    "            # ÌÜµÌï© ÌòïÌÉúÎ°ú Î≥ÄÌôò (Î™®Îì† Í∞íÏùÑ Î¨∏ÏûêÏó¥Î°ú Í∞ïÏ†ú Î≥ÄÌôò)\n",
    "            for item in task_data:\n",
    "                unified_item = {\n",
    "                    \"task_type\": task_type,\n",
    "                    \"instruction\": TASK_INSTRUCTIONS[task_type],\n",
    "                    \"context\": str(item.get(\"context\", \"ÏóÜÏùå\")),  # str() Ï∂îÍ∞Ä\n",
    "                    \"input\": str(item[\"input\"]),                  # str() Ï∂îÍ∞Ä\n",
    "                    \"output\": str(item[\"output\"])                 # str() Ï∂îÍ∞Ä\n",
    "                }\n",
    "                all_data.append(unified_item)\n",
    "            \n",
    "            print(f\"  Loaded {len(task_data)} samples for {task_type}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"  Warning: {filename} not found, skipping...\")\n",
    "            continue\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ ÏÖîÌîå\n",
    "    random.shuffle(all_data)\n",
    "    \n",
    "    print(f\"\\nTotal unified data: {len(all_data)}\")\n",
    "    print(\"Task distribution:\")\n",
    "    for task_type in set(item[\"task_type\"] for item in all_data):\n",
    "        count = sum(1 for item in all_data if item[\"task_type\"] == task_type)\n",
    "        print(f\"  {task_type}: {count}\")\n",
    "    \n",
    "    return all_data\n",
    "\n",
    "def create_balanced_dataset(all_data, min_samples_per_task=2000):\n",
    "    \"\"\"Í∞Å ÌÉúÏä§ÌÅ¨Î≥ÑÎ°ú Í∑†ÌòïÏû°Ìûå Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\"\"\"\n",
    "    task_data = {}\n",
    "    \n",
    "    # ÌÉúÏä§ÌÅ¨Î≥ÑÎ°ú Îç∞Ïù¥ÌÑ∞ Î∂ÑÎ¶¨\n",
    "    for item in all_data:\n",
    "        task_type = item[\"task_type\"]\n",
    "        if task_type not in task_data:\n",
    "            task_data[task_type] = []\n",
    "        task_data[task_type].append(item)\n",
    "    \n",
    "    # Í∞Å ÌÉúÏä§ÌÅ¨Î≥ÑÎ°ú ÏÉòÌîåÎßÅ\n",
    "    balanced_data = []\n",
    "    for task_type, data in task_data.items():\n",
    "        if len(data) >= min_samples_per_task:\n",
    "            # Ï∂©Î∂ÑÌïú Îç∞Ïù¥ÌÑ∞Í∞Ä ÏûàÏúºÎ©¥ ÎûúÎç§ ÏÉòÌîåÎßÅ\n",
    "            sampled = random.sample(data, min_samples_per_task)\n",
    "        else:\n",
    "            # Î∂ÄÏ°±ÌïòÎ©¥ Ïò§Î≤ÑÏÉòÌîåÎßÅ (Ï§ëÎ≥µ ÌóàÏö©)\n",
    "            sampled = data * (min_samples_per_task // len(data) + 1)\n",
    "            sampled = sampled[:min_samples_per_task]\n",
    "        \n",
    "        balanced_data.extend(sampled)\n",
    "        print(f\"{task_type}: {len(sampled)} samples (original: {len(data)})\")\n",
    "    \n",
    "    random.shuffle(balanced_data)\n",
    "    return balanced_data\n",
    "\n",
    "def formatting_unified_prompts_func(examples):\n",
    "    \"\"\"ÌÜµÌï© Îç∞Ïù¥ÌÑ∞Î•º ÌîÑÎ°¨ÌîÑÌä∏ ÌòïÌÉúÎ°ú Î≥ÄÌôò\"\"\"\n",
    "    task_types = examples[\"task_type\"]\n",
    "    instructions = examples[\"instruction\"] \n",
    "    contexts = examples[\"context\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    \n",
    "    texts = []\n",
    "    \n",
    "    for task_type, instruction, context, inp, out in zip(\n",
    "        task_types, instructions, contexts, inputs, outputs\n",
    "    ):\n",
    "        if not out.endswith(EOS_TOKEN):\n",
    "            out += EOS_TOKEN\n",
    "            \n",
    "        text = unified_prompt_style.format(\n",
    "            task_type=task_type,\n",
    "            instruction=instruction,\n",
    "            context=context,\n",
    "            input=inp,\n",
    "            output=out\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Ï≤òÎ¶¨\n",
    "print(\"üîÑ Loading multi-task data...\")\n",
    "all_data = load_and_process_multi_task_data()\n",
    "\n",
    "print(\"\\nüîÑ Creating balanced dataset...\")\n",
    "balanced_data = create_balanced_dataset(all_data, min_samples_per_task=2000)\n",
    "\n",
    "print(f\"\\n‚úÖ Final balanced dataset size: {len(balanced_data)}\")\n",
    "\n",
    "# Dataset Í∞ùÏ≤¥ ÏÉùÏÑ±\n",
    "dataset = Dataset.from_list(balanced_data)\n",
    "\n",
    "# ÌîÑÎ°¨ÌîÑÌä∏ ÌòïÌÉúÎ°ú Î≥ÄÌôò\n",
    "dataset = dataset.map(\n",
    "    formatting_unified_prompts_func,\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset after prompt formatting: {len(dataset)}\")\n",
    "print(\"\\nüìã Sample data preview:\")\n",
    "print(\"-\" * 50)\n",
    "print(dataset[\"text\"][0][:800] + \"...\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07427da0-5f73-4010-88e6-477d02704dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728e6602-c71a-423c-9ccd-9c278d52f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,                           # Scaling factor for LoRA\n",
    "    lora_dropout=0.05,                       # Add slight dropout for regularization\n",
    "    r=64,                                    # Rank of the LoRA update matrices\n",
    "    bias=\"none\",                             # No bias reparameterization\n",
    "    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Target modules for LoRA\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15453947-e297-4321-af2b-57f1c5943b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0a9024119b494590921e3ea8e2de60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (146822 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def filter_too_long(example):\n",
    "    tokenized = tokenizer(example[\"text\"], truncation=False, return_tensors=\"pt\")\n",
    "    return tokenized[\"input_ids\"].shape[1] <= 131072\n",
    "\n",
    "dataset = dataset.filter(filter_too_long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc151d-5b97-480e-9f4c-9ee8f9b9bee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Train dataset: 5398\n",
      "‚úÖ Eval dataset: 600\n",
      "\n",
      "üìä Train Task Distribution:\n",
      "  PATENT_EVALUATION: 1789 (33.1%)\n",
      "  PATENT_FORM: 1803 (33.4%)\n",
      "  PATENT_MODIFICATION: 1806 (33.5%)\n",
      "\n",
      "üìä Eval Task Distribution:\n",
      "  PATENT_FORM: 197 (32.8%)\n",
      "  PATENT_MODIFICATION: 193 (32.2%)\n",
      "  PATENT_EVALUATION: 210 (35.0%)\n"
     ]
    }
   ],
   "source": [
    "# 1. Dataset Î∂ÑÌï†\n",
    "split_dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(f\"‚úÖ Train dataset: {len(train_dataset)}\")\n",
    "print(f\"‚úÖ Eval dataset: {len(eval_dataset)}\")\n",
    "\n",
    "# 2. Metrics Ìï®Ïàò (optional)\n",
    "import math\n",
    "def compute_metrics(eval_preds):\n",
    "    loss = eval_preds.loss\n",
    "    return {\n",
    "        \"eval_loss\": loss,\n",
    "        \"perplexity\": math.exp(loss) if loss < 100 else float(\"inf\")\n",
    "    }\n",
    "\n",
    "# 3. Í∞Å Îç∞Ïù¥ÌÑ∞ÏÖãÏóêÏÑú ÌÉúÏä§ÌÅ¨ Î∂ÑÌè¨ ÌôïÏù∏\n",
    "def check_task_distribution(dataset_split, split_name):\n",
    "    print(f\"\\nüìä {split_name} Task Distribution:\")\n",
    "    task_counts = {}\n",
    "    for example in dataset_split:\n",
    "        # ÌîÑÎ°¨ÌîÑÌä∏ÏóêÏÑú ÌÉúÏä§ÌÅ¨ ÌÉÄÏûÖ Ï∂îÏ∂ú\n",
    "        text = example[\"text\"]\n",
    "        # Ï∂îÏ≤ú Ï†úÏô∏\n",
    "        for task_type in [\"PATENT_FORM\", \"PATENT_EVALUATION\", \"PATENT_MODIFICATION\"]:\n",
    "            if f\"Task Type: {task_type}\" in text:\n",
    "                task_counts[task_type] = task_counts.get(task_type, 0) + 1\n",
    "                break\n",
    "    \n",
    "    for task, count in task_counts.items():\n",
    "        percentage = (count / len(dataset_split)) * 100\n",
    "        print(f\"  {task}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "check_task_distribution(train_dataset, \"Train\")\n",
    "check_task_distribution(eval_dataset, \"Eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d83f1cc-7c6a-4532-9fab-20d45323937f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdcf3d86d49944a2b0820121c16c2797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/5398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5af2d1ad424826945eed2e907c91ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/5398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c541331b0e924465af7e5b954285d715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/5398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231592734c544a58b24600420ba9eaa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/5398 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c595065900f44b3a4066602a29d2134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting eval dataset to ChatML:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bb023a9f68840f1bfbacf389acd2424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to eval dataset:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb75b80a33e4e87805e84c9076e1611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2082faf1f84349ba928455f5e1eb6941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating eval dataset:   0%|          | 0/600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    eval_strategy=\"steps\",   # ‚úÖ ÌèâÍ∞Ä Ïã§Ìñâ Ï°∞Í±¥\n",
    "    eval_steps=100,                 # ‚úÖ 50 Ïä§ÌÖùÎßàÎã§ ÌèâÍ∞Ä\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=50,\n",
    "    warmup_steps=20,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,      # ÏµúÎåÄ 3Í∞ú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ Ïú†ÏßÄ\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef451e13-300e-48ea-adf9-f514fe1bff7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5900' max='8097' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5900/8097 2:11:56 < 49:08, 0.75 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.602700</td>\n",
       "      <td>0.590532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.587200</td>\n",
       "      <td>0.573379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.538700</td>\n",
       "      <td>0.566305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.606900</td>\n",
       "      <td>0.560636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.574800</td>\n",
       "      <td>0.556949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.560400</td>\n",
       "      <td>0.551803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.549200</td>\n",
       "      <td>0.550315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.532600</td>\n",
       "      <td>0.546494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.545700</td>\n",
       "      <td>0.544862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.541330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.560900</td>\n",
       "      <td>0.540293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.517300</td>\n",
       "      <td>0.538450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.542900</td>\n",
       "      <td>0.536074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.543900</td>\n",
       "      <td>0.535226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.543000</td>\n",
       "      <td>0.533162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.542800</td>\n",
       "      <td>0.530900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.567100</td>\n",
       "      <td>0.528768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.533600</td>\n",
       "      <td>0.526952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.504700</td>\n",
       "      <td>0.525673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>0.523783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.555400</td>\n",
       "      <td>0.522282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.547800</td>\n",
       "      <td>0.521176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.531500</td>\n",
       "      <td>0.520605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.536400</td>\n",
       "      <td>0.520167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.514400</td>\n",
       "      <td>0.518718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.516231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.508600</td>\n",
       "      <td>0.515392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.499900</td>\n",
       "      <td>0.515475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.495500</td>\n",
       "      <td>0.515264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.492500</td>\n",
       "      <td>0.514344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.496600</td>\n",
       "      <td>0.513500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.503400</td>\n",
       "      <td>0.513551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.491600</td>\n",
       "      <td>0.513138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.489200</td>\n",
       "      <td>0.512160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>0.510490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.487900</td>\n",
       "      <td>0.511233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.509079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.473400</td>\n",
       "      <td>0.507678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.498000</td>\n",
       "      <td>0.506498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.466100</td>\n",
       "      <td>0.505721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.485800</td>\n",
       "      <td>0.504017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.496900</td>\n",
       "      <td>0.503490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.460500</td>\n",
       "      <td>0.502380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.502677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.501538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.479800</td>\n",
       "      <td>0.500083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.500048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.450200</td>\n",
       "      <td>0.498686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.485900</td>\n",
       "      <td>0.497155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.498200</td>\n",
       "      <td>0.495605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>0.494835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.497500</td>\n",
       "      <td>0.494499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.493792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.458200</td>\n",
       "      <td>0.493300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.423500</td>\n",
       "      <td>0.496333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.419500</td>\n",
       "      <td>0.494921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.457900</td>\n",
       "      <td>0.494803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.411500</td>\n",
       "      <td>0.495708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.424900</td>\n",
       "      <td>0.495238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5900, training_loss=0.5070423585277493, metrics={'train_runtime': 7918.2825, 'train_samples_per_second': 2.045, 'train_steps_per_second': 1.023, 'total_flos': 5.613608276852736e+17, 'train_loss': 0.5070423585277493})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53cea42-4dae-4dc3-b6ae-b9a4bd2551ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('best_model/tokenizer_config.json',\n",
       " 'best_model/special_tokens_map.json',\n",
       " 'best_model/chat_template.jinja',\n",
       " 'best_model/vocab.json',\n",
       " 'best_model/merges.txt',\n",
       " 'best_model/added_tokens.json',\n",
       " 'best_model/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ïù¥ Î∂ÄÎ∂Ñ Ïù¥ÏÉÅÌï®\n",
    "# ÌïôÏäµ ÏôÑÎ£å ÌõÑ Í∞ÄÏû• Ï¢ãÏùÄ Î™®Îç∏ Ï†ÄÏû•\n",
    "trainer.save_model(\"best_model\")  # ÎîîÎ†âÌÜ†Î¶¨ Ïù¥Î¶ÑÏùÄ ÏõêÌïòÎäî ÎåÄÎ°ú\n",
    "\n",
    "# tokenizerÎèÑ Ìï®Íªò Ï†ÄÏû• (Í∂åÏû•)\n",
    "tokenizer.save_pretrained(\"best_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
