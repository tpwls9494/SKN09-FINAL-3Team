import faiss
import json
import os
import numpy as np
from sentence_transformers import SentenceTransformer

# ì„¤ì •
EMBED_MODEL = "BAAI/bge-m3"
INDEX_PATH = "vectordb/full_patent_index_bge_m3.faiss"
META_PATH = "vectordb/full_patent_metadata.jsonl"
DATA_DIR = "data"

# ëª¨ë¸ ë° ì¸ë±ìŠ¤ ë¡œë“œ
model = SentenceTransformer(EMBED_MODEL)
index = faiss.read_index(INDEX_PATH)

# ë©”íƒ€ë°ì´í„° ë¡œë”©
with open(META_PATH, "r", encoding="utf-8") as f:
    metadata = [json.loads(line) for line in f]

def get_original_text(source_file, meta):
    try:
        if meta["type"] == "íŠ¹í—ˆ":
            path = os.path.join(DATA_DIR, "processed_patents_jsonl", source_file)
        else:
            path = os.path.join(DATA_DIR, source_file)

        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                item = json.loads(line.strip())

                if meta["type"] == "íŠ¹í—ˆ":
                    if item.get("patent_id") == meta.get("patent_id") and item.get("section") == meta.get("section"):
                        if meta.get("subsection") and item.get("subsection") != meta.get("subsection"):
                            continue
                        content = " ".join(str(v) for v in item.values() if isinstance(v, str)).strip()
                        return f"[ğŸ“Œ íŠ¹í—ˆ ID: {item.get('patent_id')}]\n[ğŸ”– í•­ëª©: {item.get('section')} - {item.get('subsection')}]\n{content}"

                elif meta["type"] == "ë²•ë ¹":
                    if item.get("input") == meta.get("input") and item.get("output") == meta.get("output"):
                        return f"[ğŸ“œ ë²•ë ¹ Input: {item.get('input')}]\n[ğŸ“˜ ë²•ë ¹ Output: {item.get('output')}]\n{item.get('input')} {item.get('output')}"

                elif meta["type"] == "ê°€ì´ë“œ":
                    if item.get("gubun") == meta.get("gubun"):
                        return f"[ğŸ“˜ ê°€ì´ë“œ êµ¬ë¶„: {item.get('gubun')}]\n[ğŸ“„ ê°€ì´ë“œ ID: {item.get('id')}]\n{item.get('text')}"

    except Exception as e:
        return f"[ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}]"

    return "[ë³¸ë¬¸ ì—†ìŒ]"

def search_batch(queries, top_k=5):
    query_vecs = model.encode(queries, normalize_embeddings=True)
    results = []

    for i, query_vec in enumerate(query_vecs):
        query = queries[i]
        D, I = index.search(np.array([query_vec], dtype=np.float32), top_k)
        scores = D[0]
        indices = I[0]

        print(f"\n[ğŸ” ì§ˆì˜ {i+1}: {query}]")
        avg_score = np.mean(scores)
        print(f"[ğŸ“Š í‰ê·  ìœ ì‚¬ë„ ì ìˆ˜: {avg_score:.4f}]")

        for rank, idx in enumerate(indices):
            score = scores[rank]
            meta = metadata[idx]
            content = get_original_text(meta["source_file"], meta)

            print(f"\nğŸ“Œ Top {rank+1}: (score: {score:.4f})")
            print(json.dumps(meta, ensure_ascii=False, indent=2))
            print(f"ğŸ“„ ë³¸ë¬¸:\n{content}")

        results.append({"query": query, "avg_score": avg_score})

    return results

# ì‹¤í–‰
if __name__ == "__main__":
    query_list = [
        "ê³ ê° ìë™ ì¶”ì²œ ì•Œê³ ë¦¬ì¦˜",
        "ì¸ê³µì§€ëŠ¥ ë°œëª…ê³¼ ì‹¤ì‹œ ê°€ëŠ¥ ìš”ê±´",
        "íŠ¹í—ˆ ëª…ì„¸ì„œì˜ ì²­êµ¬í•­ ì‘ì„± ë°©ë²•",
        "íŠ¹í—ˆì²­ ì†Œì†ê¸°ê´€ ì •ì› ê¸°ì¤€"
    ]
    search_batch(query_list)
