# # ml_utils.py
# from transformers import AutoModelForCausalLM, AutoTokenizer
# import torch
# import os
# import time

# class GemmaModel:
#     def __init__(self):
#         self.model_name = "codingtree/gemma3_pass"
#         self.tokenizer = None
#         self.model = None
#         self.load_model()
    
#     def load_model(self):
#         """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
#         start_time = time.time()
#         try:
#             print(f"CPUì—ì„œ '{self.model_name}' ëª¨ë¸ ë¡œë“œ ì‹œì‘...")
            
#             # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
#             self.tokenizer = AutoTokenizer.from_pretrained(
#                 self.model_name,
#                 use_fast=False  # SentencePiece í† í¬ë‚˜ì´ì €ëŠ” use_fast=False í•„ìš”
#             )
#             print("í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ!")
            
#             # 2. CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ì„¤ì •
#             # ì •ëŸ‰í™”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
#             try:
#                 from transformers import BitsAndBytesConfig
#                 quantization_config = BitsAndBytesConfig(
#                     load_in_4bit=True,  # 4ë¹„íŠ¸ ì •ëŸ‰í™” (8ë¹„íŠ¸ë³´ë‹¤ ë” ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
#                     bnb_4bit_compute_dtype=torch.float16,
#                     bnb_4bit_quant_type="nf4"
#                 )
                
#                 # ëª¨ë¸ ë¡œë“œ (4ë¹„íŠ¸ ì •ëŸ‰í™” ì‚¬ìš©)
#                 self.model = AutoModelForCausalLM.from_pretrained(
#                     self.model_name,
#                     quantization_config=quantization_config,
#                     device_map="cpu",  # ëª…ì‹œì ìœ¼ë¡œ CPU ì‚¬ìš©
#                     torch_dtype=torch.float16,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¤„ì´ê¸°
#                     low_cpu_mem_usage=True  # CPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
#                 )
#                 print("ì •ëŸ‰í™”ëœ ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
#             except ImportError:
#                 # BitsAndBytesë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ì¼ë°˜ ëª¨ë¸ ë¡œë“œ
#                 print("ì •ëŸ‰í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¼ë°˜ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
#                 self.model = AutoModelForCausalLM.from_pretrained(
#                     self.model_name,
#                     device_map="cpu",
#                     torch_dtype=torch.float16,
#                     low_cpu_mem_usage=True
#                 )
            
#             load_time = time.time() - start_time
#             print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {load_time:.2f}ì´ˆ)")
            
#         except Exception as e:
#             print(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
#     def generate_text(self, input_text, max_length=512):  # ìƒì„± ê¸¸ì´ ì œí•œ
#         """í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. CPUì—ì„œëŠ” ê¸¸ì´ë¥¼ ì œí•œí•˜ì„¸ìš”."""
#         if not self.model or not self.tokenizer:
#             return "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
#         try:
#             print(f"í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘: '{input_text[:50]}...'")
#             start_time = time.time()
            
#             # 1. ì…ë ¥ í† í°í™”
#             inputs = self.tokenizer(input_text, return_tensors="pt")
            
#             # 2. ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•œ ìƒì„± ì„¤ì •
#             with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
#                 output = self.model.generate(
#                     inputs.input_ids,
#                     max_new_tokens=max_length,  # ìƒˆ í† í° ìˆ˜ ì œí•œ
#                     do_sample=True,
#                     temperature=0.7,
#                     top_p=0.9,
#                     pad_token_id=self.tokenizer.eos_token_id,
#                     num_return_sequences=1,  # í•˜ë‚˜ì˜ ê²°ê³¼ë§Œ ìƒì„±
#                     # CPUì—ì„œì˜ ì„±ëŠ¥ì„ ìœ„í•´ batch_size=1 ì‚¬ìš©
#                 )
            
#             # 3. ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
#             generated_text = self.tokenizer.decode(output[0], skip_special_tokens=True)
            
#             # 4. ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì œì™¸í•œ ìƒì„±ëœ ë¶€ë¶„ë§Œ ë°˜í™˜
#             result = generated_text[len(input_text):].strip()
            
#             gen_time = time.time() - start_time
#             print(f"í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {gen_time:.2f}ì´ˆ)")
            
#             return result
            
#         except Exception as e:
#             print(f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
#             return f"í…ìŠ¤íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"
    
#     def evaluate_text(self, input_text):
#         """í…ìŠ¤íŠ¸ë¥¼ í‰ê°€í•©ë‹ˆë‹¤."""
#         # í‰ê°€ëŠ” ì§§ì€ ê²°ê³¼ë§Œ í•„ìš”í•˜ë¯€ë¡œ ë” ì ì€ í† í° ìƒì„±
#         try:
#             eval_prompt = f"ë‹¤ìŒ íŠ¹í—ˆ ì´ˆì•ˆì„ í‰ê°€í•˜ì„¸ìš”:\n\n{input_text[:1000]}...\n\ní‰ê°€ ê²°ê³¼:"
#             evaluation_result = self.generate_text(eval_prompt, max_length=256)
#             return f"ğŸ§  AI í‰ê°€ ê²°ê³¼: {evaluation_result}"
#         except Exception as e:
#             return f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# # ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
# patent_generator = GemmaModel()

# # í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì•ˆë‚´ ì¶œë ¥
# print("\nëª¨ë¸ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš° ë‹¤ìŒ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”:")
# print("pip install transformers torch sentencepiece accelerate bitsandbytes")